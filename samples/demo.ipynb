{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**  \n",
    "\n",
    "1. Import ``align`` and its dependencies.  \n",
    "1. Import ``tagfixer`` module from wherever you have saved it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Import dependencies\n",
    "import align, os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "### Import tagfixer module\n",
    "import sys\n",
    "tagfixerpath = '/Users/ethan/Documents/GitHub/align-tag-fixer/src/'\n",
    "sys.path.append(tagfixerpath)\n",
    "import tagfixer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "Set up folder structure  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'my-project'\n",
    "path_to_google_news_vectors = '/Users/ethan/Documents/ALIGN/archive/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "PROJECT_FOLDER = os.path.join(BASE_PATH,\n",
    "                              project_name)\n",
    "if not os.path.exists(PROJECT_FOLDER):\n",
    "    os.makedirs(PROJECT_FOLDER)\n",
    "\n",
    "TRANSCRIPTS = os.path.join(PROJECT_FOLDER,\n",
    "                                   'input/')\n",
    "if not os.path.exists(TRANSCRIPTS):\n",
    "    os.makedirs(TRANSCRIPTS)\n",
    "\n",
    "\n",
    "PREPPED_TRANSCRIPTS = os.path.join(PROJECT_FOLDER,\n",
    "                                   'prepped/')\n",
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)\n",
    "\n",
    "ANALYSIS_READY = os.path.join(PROJECT_FOLDER,\n",
    "                              'analysis/')\n",
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)\n",
    "\n",
    "SURROGATE_TRANSCRIPTS = os.path.join(PROJECT_FOLDER,\n",
    "                                     'surrogate/')\n",
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)\n",
    "\n",
    "\n",
    "OPTIONAL_PATHS = os.path.join(PROJECT_FOLDER,\n",
    "                             'optional_directories/')\n",
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)\n",
    "\n",
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            path_to_google_news_vectors)\n",
    "if os.path.exists(PRETRAINED_INPUT_FILE) == False:\n",
    "    warnings.warn('Google News vector not found at the specified '\n",
    "                      'location. Please update the file path with '\n",
    "                      'the .bin file that can be accessed here: '\n",
    "                      'https://code.google.com/archive/p/word2vec/ '\n",
    "                      '- Alternatively, comment out the `PRETRAINED_INPUT_FILE` information')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "run ``tagfixer.chat2penn``. Arguments are:\n",
    "\n",
    "1. ``cut_file_folder``: a string with the path to a folder that contains all the .cut files with custom lexemes\n",
    "2. ``output_directory``: a string with the path to a folder where you want the function to place a .csv file with the custom lexemes, the original chat POS tags, and the corresponding Penn POS tags\n",
    "3. ``lookup_table``: a string with the path to a csv file which contains the translation from chat to penn tag formats. Must contain two columns: \"chat_POS\" and \"penn_POS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathin='/Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/cut_files/'\n",
    "pathout='/Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/cut_files/'\n",
    "lookup_table='/Users/ethan/Documents/GitHub/align-tag-fixer/res/chat_POS_lookup-table.csv'\n",
    "\n",
    "tagfixer.chat2penn(cut_file_folder=pathin, \n",
    "                   output_directory=pathout,\n",
    "                   lookup_table=lookup_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "Instead of the ALIGN ``prepare_transcripts`` function, use the tagfixer ``prepare_transcripts`` version.\n",
    "\n",
    "- make sure to set ``run_spell_check`` to ``False``\n",
    "- set ``custom_dictionary`` to ``True``. If it is set to ``False``, ``tagfixer.prepare_transcrips`` should work just like ``align.prepare_transcripts```\n",
    "- set ``path_to_custom_dictionary`` to the path to the csv file created by ``tagfixer.chat2penn``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/input/Dyad3-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/input/Dyad2-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/input/Dyad1-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/input/Dyad4-time1-condTD.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "custom_dictionary = '/Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/cut_files/_custom_chat_tags.csv'\n",
    "\n",
    "model_store = tagfixer.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        input_as_directory=True,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        run_spell_check=False,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        filler_regex_and_list=False,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                            ## if you want to run the Stanford POS tagger, be sure to set `add_stanford_tags` to `True` and uncomment the next two lines\n",
    "                            # stanford_pos_path=STANFORD_POS_PATH,\n",
    "                            # stanford_language_path=STANFORD_LANGUAGE,\n",
    "                        save_concatenated_dataframe=True,\n",
    "                        custom_dictionary=True,\n",
    "                        path_to_custom_dictionary=custom_dictionary)\n",
    "                        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**\n",
    "\n",
    "You can check to see what custom words were identified in the transcripts, and in which files. With a bit more work, this could be modified to show what Penn tag they have been assigned, but at least with this information you can search for some of the words in the prepped transcripts, and check that everything seems correct.  \n",
    "\n",
    "- ``input_folder`` is a string with the path to the prepped transcript files\n",
    "- ``path_to_custom_dictionary`` is a string with the path to the csv file created by ``tagfixer.chat2penn``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/prepped/'\n",
    "custom_dictionary = '/Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/cut_files/_custom_chat_tags.csv'\n",
    "\n",
    "cus_words = tagfixer.find_custom_words(input_folder, path_to_custom_dictionary=custom_dictionary)\n",
    "cus_words.to_csv('cus_words.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Steps 6 and 7**\n",
    "\n",
    "From here on, you're back to the original ALIGN workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(PROJECT_FOLDER,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/prepped/Dyad3-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/prepped/Dyad2-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/prepped/Dyad1-time1-condTD.txt\n",
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/prepped/Dyad4-time1-condTD.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_real,convo_real] = align.calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/ethan/Documents/GitHub/align-tag-fixer/samples/my-project/surrogate/surrogate_run-1688125107.5451908/SurrogatePair-time1A-time1B-condTD.txt\n"
     ]
    }
   ],
   "source": [
    "[turn_surrogate,convo_surrogate] = align.calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    #id_separator='',\n",
    "                                    dyad_label='time',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
